<!DOCTYPE html>
<html lang="en">
<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Monte Carlo Tree Search (MCTS) - CMSC818B Mini Project 1</title>
    <link rel="stylesheet" href="wiki.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/addons/p5.dom.min.js"></script>
</head>
<body>
    <header>
        <h1>Monte Carlo Tree Search (MCTS)</h1>
        <h2>CMSC818B  - Mini Project -1</h2>
        <h3>Team Members: Sai Jagadeesh Muralikrishnan, Varun Lakshmanan, Nitish Ravichandran Raveendran</h3>
        <p>A Simple Guide for Everyone</p>
    </header>
    
    <nav>
        <ul>
            <li><a href="#summary">Summary</a></li>
            <li><a href="#formal-definition">Formal Definition</a></li>
            <li><a href="#mcts-visualization">MCTS Animation</a></li>
            <li><a href="#key-results">Key Results</a></li>
            <li><a href="#robotics">MCTS in Decision Making Robotics</a></li>
            <li><a href="#variants">Variants</a></li>
            <li><a href="#pseudocode">Pseudocode</a></li>
            <li><a href="#applications">Applications</a></li>
            <li><a href="#open-questions">Open Research Questions</a></li>
            <li><a href="#references">References</a></li>

        </ul>
    </nav>
    
    <section id="summary">
        <h2>Summary of Monte Carlo Tree Search</h2>
        <p>Monte-Carlo Tree Search (MCTS) is a powerful algorithm used to make decisions by exploring potential future outcomes in a tree-like structure. Imagine you're playing a game like chess; there are many possible moves you could make, and each move leads to a different set of possibilities. MCTS helps to identify the best move by simulating different potential sequences of actions (paths) and assessing their outcomes. The algorithm uses random sampling of these paths to estimate the potential value of different actions, guiding it towards exploring more promising options. It’s like trying different routes on a map to see which path leads to the best destination while gradually focusing more on the most promising routes.</p>
        <p>The strength of MCTS lies in its balance between exploration (trying out less-known possibilities) and exploitation (focusing on the known good moves). It continuously updates its strategy based on new information from these simulations, refining the search towards the most advantageous actions. This makes MCTS suitable for problems where the search space is vast and it's impossible to compute all possible outcomes, such as complex board games or real-time decision-making tasks.</p>
    </section>
    
    <section id="formal-definition">
        <h2>Formal Definition of Monte Carlo Tree Search</h2>
        <p>Monte Carlo Tree Search is a heuristic search which focuses only on the most promising nodes of the tree, where the value of these nodes is formed based on the outcomes of a number of simulations.</p>
        <p>The Monte Carlo Tree Search is done in multiple steps:</p>
        <h3>1. Selection:</h3>
        <p>This process is done by traversing the tree from the root node until finding a node which contains unexplored children and is not in a terminal state. The child node is selected based on the upper confidence trees that follow the Upper Confidence Bound (UCB) rule, which is defined by the formula:</p>
        <div style="background: #f9f9f9; padding: 15px; border-radius: 5px;">
            <strong>
                \[ \max_{w \in \text{children}(v)} \left( Q(w) + C \sqrt{\frac{2 \ln N(v)}{N(w)}} \right) \]
            </strong>
        </div>
        <p>Where:</p>
        <ul>
            <li><strong>w</strong> – Child Node</li>
            <li><strong>v</strong> – Parent Node</li>
            <li><strong>Q(w)</strong> – Average reward or value of child node <em>w</em></li>
            <li><strong>N(v)</strong> – Number of times node <em>v</em> is visited</li>
            <li><strong>N(w)</strong> – Number of times child node <em>w</em> is visited</li>
            <li><strong>C</strong> – Constant for balancing exploration and exploitation</li>
        </ul>
        <p>The first term \( Q(w) \) favors exploitation where the nodes have performed well historically, and the second term \( C \sqrt{\frac{2 \ln N(v)}{N(w)}} \) favors exploration where the nodes are less explored. The node with the higher UCB value is selected.</p>
        
        <h3>2. Expansion:</h3>
        <p>Here, one or more new nodes are added to the node which was selected during the selection process. The new node is called the child node or leaf node.</p>
        
        <h3>3. Simulation:</h3>
        <p>A random simulation (or "rollout") is performed from the newly expanded node. This involves selecting actions according to a default policy until a terminal state is reached, where a reward is obtained.</p>
        
        <h3>4. Backpropagation:</h3>
        <p>Once the terminal state is reached, backpropagation is performed from the leaf node back to the root node while updating the \( Q(v) \) for all the nodes and incrementing \( N(v) \) along the path. Here, \( Q(v) \) is the sum of all the simulation outcomes in the subtree.</p>
    </section>
    
    <section id="mcts-visualization">
        <h2>Monte Carlo Tree Search Visualization</h2>
        <p>This animation demonstrates the Monte Carlo Tree Search (MCTS) process. It illustrates the four main steps: Selection, Expansion, Simulation, and Backpropagation, showing how the algorithm navigates through the search tree to make decisions.</p>
        <img src="mcts.gif" alt="MCTS Visualization" style="max-width:100%; height:auto;">
        <p>In the animation:</p>
        <ul>
            <li><strong>Selection:</strong> The algorithm selects the most promising node, balancing exploration and exploitation.</li>
            <li><strong>Expansion:</strong> A one or more nodes are added to expand the tree if not already fully explored.</li>
            <li><strong>Simulation:</strong> A random simulation is run from the new node to determine potential outcomes.</li>
            <li><strong>Backpropagation:</strong> The simulation results are propagated back through the nodes, updating their values.</li>
        </ul>
    </section>
    
    <section id="key-results">
        <h2>Overview of Key Results</h2>
        <p>Monte-Carlo Tree Search (MCTS) has demonstrated significant success across various domains, particularly in AI-driven game playing. One of the most remarkable achievements is in the game of Go, where traditional search techniques struggle due to the immense search space. The success of<strong> AlphaGo, which utilized MCTS in combination with deep neural networks, showcased how MCTS can guide AI to perform at a superhuman level</strong>. By simulating different move sequences and evaluating the outcomes, MCTS enabled AlphaGo to select optimal strategies and defeat world champions.</p>
        <p>Beyond games, MCTS has proven effective in other decision-making tasks, such as <strong>robotics and real-time optimization</strong>. In robotic path planning, for instance, MCTS helps robots navigate through dynamic environments by simulating different paths and choosing the one with the highest likelihood of success. This adaptability to different problem domains is one of the key reasons why MCTS is widely used, <strong>providing near-optimal solutions in situations where exhaustive search is impractical</strong>.</p>
        <p>MCTS also plays a crucial role in reinforcement learning (RL), where it is used to improve decision-making capabilities. Algorithms like <strong>AlphaZero and MuZero incorporate MCTS to simulate future actions and evaluate potential outcomes, enabling the agents to learn and refine their strategies over time</strong>. This integration has shown that MCTS can be effectively combined with deep learning techniques to tackle complex, high-dimensional problems across a variety of fields, including robotics, optimization, and strategic planning.</p>
    </section>
    
    
    <section id="robotics">
        <h2>How MCTS Relates to Decision-Making for Robotics</h2>
        <p><strong>Monte-Carlo Tree Search (MCTS)</strong> is widely used in robotics for <strong>sequential decision-making tasks</strong>, especially in scenarios where the robot needs to plan its actions under <strong>uncertainty</strong>. In robotics, decision-making often involves <strong>navigating a dynamic environment</strong>, choosing actions that optimize certain objectives (such as reaching a goal, minimizing energy use, or avoiding obstacles). MCTS helps robots make these decisions by <strong>simulating different action sequences</strong> and <strong>evaluating their potential outcomes</strong>. It is particularly beneficial in situations where the robot must <strong>decide on the best course of action in real-time</strong>, as MCTS can dynamically adapt its strategy based on new observations and information gained through simulations.</p>
        
        <p>One example is <strong>robotic path planning in uncertain environments</strong>, such as a robot navigating through a cluttered room or an unmapped outdoor area. Here, MCTS can <strong>simulate potential paths</strong> and <strong>predict the likelihood of success</strong> (e.g., avoiding obstacles) for each path. The robot then <strong>chooses the most promising path</strong> based on these simulations, <strong>updating its plan</strong> if new obstacles are detected. In more complex tasks, like <strong>robotic manipulation</strong>, MCTS can help the robot decide the <strong>sequence of movements</strong> needed to grasp an object or assemble a structure, considering the uncertainties in the environment and the robot's own motion.</p>
        
        <p>Another important application is in <strong>multi-robot coordination</strong>, where MCTS helps in <strong>deciding the optimal strategy</strong> for each robot in a group to achieve a collective goal. For instance, in <strong>search and rescue operations</strong>, multiple drones or robots may be used to explore different areas or search for survivors. MCTS can be used to <strong>decide which regions each robot should explore</strong> by simulating different allocation strategies and choosing the one that maximizes coverage and minimizes overlap. Similarly, in industrial settings, robots working together on a task, such as <strong>transporting objects in a warehouse</strong>, can use MCTS to <strong>optimize their paths</strong> and <strong>reduce potential collisions or delays</strong>.</p>
    </section>
    
    
    
    <section id="variants">
        <h2>Variants of MCTS</h2>
        <p>Monte Carlo Tree Search (MCTS) has several variants that adapt the basic approach to different contexts and improve its efficiency, scalability, or applicability. Here are some of the most common variants and the key differences between them:</p>
    
        <h3>1. Basic MCTS (Standard)</h3>
        <p>The <strong>basic MCTS algorithm</strong> consists of four steps: <strong>Selection, Expansion, Simulation,</strong> and <strong>Backpropagation</strong>. The original form of MCTS follows a standard implementation of these steps, with nodes selected based on criteria like the <strong>Upper Confidence Bound for Trees (UCT)</strong>.</p>
    
        <h3>2. UCT (Upper Confidence Bound for Trees)</h3>
        <p><strong>Key Feature:</strong> UCT is a specific method of node selection within MCTS that <strong>balances exploration (trying less explored nodes) and exploitation (focusing on promising nodes)</strong>.</p>
        <p><strong>Difference:</strong> UCT uses a formula based on the <strong>Upper Confidence Bound (UCB)</strong> to decide which node to visit, ensuring a balance between gaining more information about lesser-known parts of the tree and focusing on the current best options.</p>
        <p><strong>Advantage:</strong> UCT helps the algorithm focus on the <strong>best moves while still occasionally trying new ones</strong> to avoid missing out on potential opportunities.</p>
    
        <h3>3. RAVE (Rapid Action Value Estimation)</h3>
        <p><strong>Key Feature:</strong> RAVE is a technique used to <strong>speed up the convergence of MCTS</strong> by sharing information across similar states.</p>
        <p><strong>Difference:</strong> It estimates the value of a move not just from the context in which it has been played, but also from <strong>other contexts in the tree where the move was possible</strong>. It gives earlier and more reliable estimates of move quality.</p>
        <p><strong>Advantage:</strong> RAVE accelerates MCTS by <strong>quickly propagating information about move effectiveness</strong>, reducing the number of simulations required to get accurate results. It is especially useful in the early stages of exploration.</p>
    
        <h3>4. AMAF (All Moves As First)</h3>
        <p><strong>Key Feature:</strong> This is an estimation method similar to RAVE. It assumes that a <strong>move should have similar value</strong> regardless of the point in a sequence in which it is made.</p>
        <p><strong>Difference:</strong> AMAF works by treating <strong>moves made later in a simulation as if they were played at the first opportunity</strong>. This provides more data for evaluating moves, even though the move's position may have differed.</p>
        <p><strong>Advantage:</strong> This helps the MCTS <strong>converge faster by effectively increasing the sample size</strong> of move evaluations.</p>
    
        <h3>5. Progressive Widening</h3>
        <p><strong>Key Feature:</strong> Limits the <strong>branching factor</strong> by adding new children incrementally as more simulations are performed.</p>
        <p><strong>Difference:</strong> Instead of expanding every possible move immediately, <strong>progressive widening adds moves to the search tree gradually</strong>, with a selection bias towards promising actions.</p>
        <p><strong>Advantage:</strong> This helps <strong>control the number of child nodes</strong> and improves computational efficiency, especially in games with a large branching factor where exploring all possible moves at once would be impractical.</p>
    
        <h3>6. Progressive Bias</h3>
        <p><strong>Key Feature:</strong> Uses <strong>heuristic information</strong> to bias the selection of nodes within the MCTS process.</p>
        <p><strong>Difference:</strong> Progressive bias alters the <strong>UCT formula</strong> to include heuristic scores for different nodes, making it more likely that <strong>promising moves are selected earlier</strong> in the search process.</p>
        <p><strong>Advantage:</strong> When domain knowledge is available (like heuristic evaluation of a board position), this variant helps the search to <strong>converge faster by prioritizing likely good moves</strong> based on that knowledge.</p>
    
        <h3>7. P-UCT (Policy Upper Confidence Bound for Trees)</h3>
        <p><strong>Key Feature:</strong> Uses a <strong>policy network</strong> to guide the selection of nodes.</p>
        <p><strong>Difference:</strong> P-UCT leverages a <strong>policy network (from deep learning)</strong> to bias MCTS towards moves that have higher prior probabilities, instead of relying purely on UCB for exploration and exploitation.</p>
        <p><strong>Advantage:</strong> The policy network helps MCTS <strong>make more informed decisions</strong> by guiding the search, leading to fewer simulations needed to identify strong moves. It’s used in implementations like AlphaGo.</p>
    
        <h3>8. Nested Monte Carlo Search</h3>
        <p><strong>Key Feature:</strong> Uses a <strong>hierarchy of Monte Carlo searches</strong> to refine the evaluation of moves.</p>
        <p><strong>Difference:</strong> In a nested search, a Monte Carlo search at a higher level calls another Monte Carlo search at a lower level for evaluating nodes.</p>
        <p><strong>Advantage:</strong> This can <strong>increase the precision of move evaluation</strong> and lead to stronger decision-making, particularly in puzzles or games that require precise evaluation at many levels of depth.</p>
    
        <h3>9. Parallel MCTS</h3>
        <p><strong>Key Feature:</strong> <strong>Parallelizes the MCTS algorithm</strong> across multiple processors or threads to speed up simulations.</p>
        <p><strong>Difference:</strong> Variants like <strong>root parallelization</strong> (starting multiple searches from the root node) or <strong>leaf parallelization</strong> (running rollouts in parallel) are used to speed up the MCTS process.</p>
        <p><strong>Advantage:</strong> Significantly <strong>speeds up the search</strong> by using multiple CPUs or GPUs, enabling more simulations to be run in the same amount of time.</p>
    
        <h3>10. Transpositions (Transposition Tables)</h3>
        <p><strong>Key Feature:</strong> Transposition tables <strong>store information about game states</strong> that have been visited multiple times.</p>
        <p><strong>Difference:</strong> MCTS with transposition tables avoids <strong>re-exploring nodes</strong> that represent the same game state but are reached via different sequences of moves.</p>
        <p><strong>Advantage:</strong> Reduces redundant calculations, especially in games with a <strong>high number of transpositions</strong> (i.e., equivalent states reachable through different move orders).</p>
    </section>
    
    
    <section id="pseudocode">
        <h2>Pseudocode for Monte Carlo Tree Search (MCTS)</h2>
        <p>The pseudocode below follows the four main steps of MCTS: Selection, Expansion, Simulation (Rollout), and Backpropagation. It explains how the algorithm navigates the search tree, adds new nodes, simulates outcomes, and updates the tree based on simulation results.</p>
        <pre>
    <code># Main function for the Monte Carlo Tree Search
    def monte_carlo_tree_search(root):
        while resources_left(time, computational_power):
            # Step 1: Selection
            leaf = traverse(root)
            
            # Step 2: Expansion
            if not is_terminal(leaf):
                leaf = expand(leaf)
            
            # Step 3: Simulation (Rollout)
            simulation_result = rollout(leaf)
            
            # Step 4: Backpropagation
            backpropagate(leaf, simulation_result)
            
        # Return the best child node based on the number of visits
        return best_child(root)
    
    # Function for node traversal (Selection)
    def traverse(node):
        while fully_expanded(node) and not is_terminal(node):
            node = best_uct(node)
        # Return the node that has unexplored children or is terminal
        return node
    
    # Function for expanding the tree (Expansion)
    def expand(node):
        # Add a new child node from the list of unexplored children
        child = pick_unvisited(node.children)
        node.children.append(child)
        return child
    
    # Function for the result of the simulation (Rollout)
    def rollout(node):
        # Simulate until a terminal state is reached
        while not is_terminal(node):
            node = rollout_policy(node)
        # Return the outcome of the simulation
        return result(node)
    
    # Function for randomly selecting a child node (Rollout Policy)
    def rollout_policy(node):
        return pick_random(node.children)
    
    # Function for backpropagation (Backpropagation)
    def backpropagate(node, result):
        # Update the stats of the node and propagate the result up the tree
        while node is not None:
            node.stats = update_stats(node, result)
            node = node.parent
    
    # Function for selecting the best child based on the UCT value
    def best_uct(node):
        # Use the UCB formula to select the child with the highest score
        return max(node.children, key=uct_value)
    
    # Function to calculate the UCT value
    def uct_value(node):
        # Q(node) is the average reward, N(parent) is the number of visits to the parent, N(node) is the number of visits to this node
        C = exploration_constant
        return node.Q + C * math.sqrt(2 * math.log(node.parent.N) / node.N)
    
    # Function for selecting the best child (Best Child)
    def best_child(node):
        # Pick the child with the highest number of visits
        return max(node.children, key=lambda child: child.N)
    </code>
        </pre>
        <p>This pseudocode ensures that the MCTS process adheres to the formal definition, covering the steps of Selection, Expansion, Simulation, and Backpropagation. Each function is designed to perform a specific task within these steps, making it a comprehensive implementation of the algorithm.</p>
    </section>
    
    <section id="applications">
        <h2>Important Applications of MCTS in RL</h2>
        
        <h3>AlphaGo and AlphaGo Zero</h3>
        <p><strong>AlphaGo</strong> is perhaps the most famous application of MCTS, where it was used in combination with deep neural networks to conquer the ancient board game Go. Go is notorious for its vast search space—far exceeding that of chess—making traditional brute-force approaches infeasible. In AlphaGo, <strong>MCTS simulated different sequences of moves to evaluate their potential outcomes</strong>, while deep neural networks predicted the probability of winning from each state. The integration of MCTS enabled the system to <strong>efficiently explore promising moves by balancing exploration and exploitation</strong>. This approach allowed AlphaGo to achieve superhuman performance, culminating in a <strong>victory against world champion Lee Sedol in 2016</strong>, marking a significant breakthrough in AI research.</p>
        <p>AlphaGo Zero, a successor to AlphaGo, achieved superhuman performance without using human game data. Instead, it learned entirely from self-play using reinforcement learning, starting from random play. This version further optimized the use of MCTS in conjunction with deep learning by employing a single neural network to represent both policy and value, thereby simplifying the architecture while improving performance.</p>
        <figure>
            <img src="alpha_go.png" alt="AlphaGo Visualization" title="AlphaGo Visualization" style="max-width:100%; height:auto;">
            <figcaption>Figure 1: Visualization of AlphaGo's strategy using Monte-Carlo Tree Search in the game of Go.</figcaption>
        </figure>
        <ol>
            <li><strong>Selection</strong>: During the selection phase, AlphaGo selects the most promising node to explore by calculating the upper confidence bound for trees (UCT). The UCT balances between exploiting known high-reward moves (high Q-values) and exploring less-visited nodes (guided by the exploration term). The move with the highest value is chosen for further exploration.</li>
            <li><strong>Expansion</strong>: In this phase, the selected node is expanded by adding new child nodes representing potential future moves. This step increases the search tree's depth and breadth, allowing AlphaGo to consider more potential game outcomes.</li>
            <li><strong>Evaluation</strong>: The newly expanded node is then evaluated by running a rollout (simulated play) to predict the outcome from that position. Deep neural networks are used to estimate the value of the game state and the policy (likelihood of each move).</li>
            <li><strong>Backup</strong>: Finally, the results of the evaluation are propagated back through the tree, updating the Q-values for all the nodes along the path. This step helps AlphaGo refine its estimates of the action values and select more promising moves in subsequent simulations.</li>
        </ol>
    
        <h3>AlphaZero</h3>
        <p><strong>AlphaZero</strong>, an evolution of AlphaGo Zero, generalizes the approach to master multiple games—such as chess, shogi, and Go—without any domain-specific knowledge. In contrast to traditional game-playing programs that rely on handcrafted evaluation functions and domain-specific adaptations, <strong>AlphaZero started with no prior knowledge and learned purely through self-play</strong>, given only the game rules. The same algorithm and neural network architecture were applied to all three games, demonstrating that a <strong>general-purpose reinforcement learning algorithm can achieve superhuman performance across multiple challenging games</strong>. MCTS guided the learning process by simulating different moves during training, evaluating the outcomes, and <strong>using the results to improve the policy and value networks</strong>.</p>
    
        <h3>MuZero</h3>
        <p><strong>MuZero</strong> further advances the application of MCTS by <strong>learning not only the optimal policy but also the model of the environment</strong>. It combines MCTS with <strong>model-based reinforcement learning</strong>, where the agent learns to predict the environment's dynamics (transitions and rewards) through experience. This allows MuZero to be applied to a broader range of problems, including classic board games like chess, Go, and shogi, as well as <strong>visually rich environments in Atari games</strong>. By <strong>learning a model of the environment</strong>, MuZero can simulate future outcomes using MCTS even when the underlying rules are not known, making it a <strong>powerful tool for decision-making in complex, real-world scenarios</strong>.</p>
        <figure>
            <img src="1.png" alt="MuZero Diagram" title="MuZero Diagram" style="max-width:100%; height:auto;">
            <figcaption>Figure 2: Diagram showing how MuZero uses Monte-Carlo Tree Search (MCTS) for planning and learning.</figcaption>
        </figure>
        <p>In MuZero, MCTS is used to plan the agent's actions by simulating different sequences of actions and their corresponding outcomes. As shown in the diagram, MuZero predicts the future dynamics of the environment (such as the next state and reward) based on its learned model. MCTS is used to evaluate possible actions by simulating multiple steps ahead, with the agent predicting state transitions, rewards, and policy values. These predictions are then used to expand the search tree and choose the most promising actions. The process involves iterating over simulations to refine the estimates of action values, helping MuZero make more informed decisions in complex environments where the exact rules may not be fully known.</p>
        
        <h3>TreeQN and ATreeC</h3>
        <p>In the realm of deep reinforcement learning, <strong>TreeQN and ATreeC</strong> are algorithms that incorporate MCTS into the training process to enhance decision-making capabilities. In TreeQN, <strong>MCTS is integrated with Q-learning</strong> to build a tree of future states, allowing the agent to <strong>simulate potential future scenarios</strong> to improve its action-value estimates. Similarly, <strong>ATreeC extends actor-critic methods by using MCTS to perform lookahead planning during training</strong>. These methods demonstrate how <strong>incorporating tree search within RL frameworks</strong> can help the agent learn more effective policies, particularly in <strong>environments with long-term dependencies or sparse rewards</strong>, where standard RL techniques might struggle.</p>
    </section>
    
    
    
    
    <section id="open-questions">
        <h2>Open Research Questions</h2>
        <p>Even though MCTS has proven effective, there are still open questions for research:</p>
        <ul>
            <li><strong>Scalability:</strong> How can we make MCTS scale effectively when dealing with extremely large decision spaces?</li>
            <li><strong>Handling Uncertainty:</strong> How can MCTS better handle situations with high uncertainty, such as real-world applications where outcomes are unpredictable?</li>
            <li><strong>Integration with Learning:</strong> Combining MCTS with reinforcement learning has shown promise (e.g., AlphaGo). The challenge is how to generalize this integration for broader applications.</li>
        </ul>
    </section>
    
    <section id="references">
        <h2>References</h2>
        
        <h3>MCTS Variants and Formal Definition</h3>
        <ul>
            <li>Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., ... & Colton, S. (2012). "A Survey of Monte Carlo Tree Search Methods." <em>IEEE Transactions on Computational Intelligence and AI in Games</em>, 4(1), 1-43.</li>
            <li>Kocsis, L., & Szepesvári, C. (2006). "Bandit based Monte-Carlo Planning." In <em>European Conference on Machine Learning (ECML)</em> (pp. 282-293). Springer, Berlin, Heidelberg.</li>
            <li>Coulom, R. (2006). "Efficient selectivity and backup operators in Monte-Carlo tree search." In <em>International Conference on Computers and Games</em> (pp. 72-83). Springer, Berlin, Heidelberg.</li>
        </ul>
        
        <h3>AlphaGo</h3>
        <ul>
            <li>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). "Mastering the game of Go with deep neural networks and tree search." <em>Nature</em>, 529(7587), 484-489.</li>
            <li>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2017). "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm." <em>arXiv preprint arXiv:1712.01815</em>.</li>
        </ul>
        
        <h3>AlphaZero</h3>
        <ul>
            <li>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2018). "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." <em>Science</em>, 362(6419), 1140-1144.</li>
        </ul>
        
        <h3>MuZero</h3>
        <ul>
            <li>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., & Silver, D. (2020). "Mastering Atari, Go, Chess and Shogi by planning with a learned model." <em>Nature</em>, 588(7839), 604-609.</li>
        </ul>
        
        <h3>TreeQN and ATreeC</h3>
        <ul>
            <li>Farquhar, G., Rocktaeschel, T., Igl, M., & Whiteson, S. (2018). "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning." <em>arXiv preprint arXiv:1710.11417</em>.</li>
        </ul>
    </section>
    
    

    <footer>
        <p>CMSC818B Mini Project 1 - Team Members: Sai Jagadeesh Muralikrishnan, Varun Lakshmanan, Nitish Ravichandran Raveendran</p>
        <p>&copy; 2024 Monte Carlo Tree Search Guide</p>
    </footer>
</body>
</html>
